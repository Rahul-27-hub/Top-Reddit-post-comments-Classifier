{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f972ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files into separate DataFrames\n",
    "posts_df= pd.read_csv('Top_posts.csv')\n",
    "comments_df = pd.read_csv('Top_posts_comments.csv')\n",
    "\n",
    "# Merge the two DataFrames on the post ID column\n",
    "merged_df = pd.merge(posts_df, comments_df, on='post_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96543e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       post_id                                         post_title  \\\n",
      "0       gh1dj9  [Project] From books to presentations in 10s w...   \n",
      "1       gh1dj9  [Project] From books to presentations in 10s w...   \n",
      "2       gh1dj9  [Project] From books to presentations in 10s w...   \n",
      "3       gh1dj9  [Project] From books to presentations in 10s w...   \n",
      "4       gh1dj9  [Project] From books to presentations in 10s w...   \n",
      "...        ...                                                ...   \n",
      "223163  efk5n3  Tesla's Neural Net can now identify red and gr...   \n",
      "223164  efk5n3  Tesla's Neural Net can now identify red and gr...   \n",
      "223165  efk5n3  Tesla's Neural Net can now identify red and gr...   \n",
      "223166  efk5n3  Tesla's Neural Net can now identify red and gr...   \n",
      "223167  efk5n3  Tesla's Neural Net can now identify red and gr...   \n",
      "\n",
      "              subreddit                                           post_url  \\\n",
      "0       MachineLearning                    https://v.redd.it/v492uoheuxx41   \n",
      "1       MachineLearning                    https://v.redd.it/v492uoheuxx41   \n",
      "2       MachineLearning                    https://v.redd.it/v492uoheuxx41   \n",
      "3       MachineLearning                    https://v.redd.it/v492uoheuxx41   \n",
      "4       MachineLearning                    https://v.redd.it/v492uoheuxx41   \n",
      "...                 ...                                                ...   \n",
      "223163       artificial  https://www.teslarati.com/tesla-holiday-update...   \n",
      "223164       artificial  https://www.teslarati.com/tesla-holiday-update...   \n",
      "223165       artificial  https://www.teslarati.com/tesla-holiday-update...   \n",
      "223166       artificial  https://www.teslarati.com/tesla-holiday-update...   \n",
      "223167       artificial  https://www.teslarati.com/tesla-holiday-update...   \n",
      "\n",
      "       flair_text  score  comments  upvote_ratio            date-time  year  \\\n",
      "0         Project   7798       186          0.99  2020-05-10 13:19:54  2020   \n",
      "1         Project   7798       186          0.99  2020-05-10 13:19:54  2020   \n",
      "2         Project   7798       186          0.99  2020-05-10 13:19:54  2020   \n",
      "3         Project   7798       186          0.99  2020-05-10 13:19:54  2020   \n",
      "4         Project   7798       186          0.99  2020-05-10 13:19:54  2020   \n",
      "...           ...    ...       ...           ...                  ...   ...   \n",
      "223163        NaN     80        10          0.89  2019-12-25 18:50:50  2019   \n",
      "223164        NaN     80        10          0.89  2019-12-25 18:50:50  2019   \n",
      "223165        NaN     80        10          0.89  2019-12-25 18:50:50  2019   \n",
      "223166        NaN     80        10          0.89  2019-12-25 18:50:50  2019   \n",
      "223167        NaN     80        10          0.89  2019-12-25 18:50:50  2019   \n",
      "\n",
      "                                                  comment  \n",
      "0       Twitter thread: [https://twitter.com/cyrildiag...  \n",
      "1                                            The future ðŸ¤¯  \n",
      "2       Simple yet very useful. Thank you for sharing ...  \n",
      "3       Almost guaranteed, Apple will copy your idea i...  \n",
      "4       Ohh the nightmare of making this into a stable...  \n",
      "...                                                   ...  \n",
      "223163  LiDAR is mot powerful sensor for the auto driv...  \n",
      "223164  So it can now idenrify traffic lights? Musk pr...  \n",
      "223165                                      Hydranet bro!  \n",
      "223166         It even shows flashing yellow turn arrows.  \n",
      "223167    Ya just saw karpathy talk on hydra and pytorch.  \n",
      "\n",
      "[223168 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36154018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223168, 11)\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4eae75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  post_id                                         post_title        subreddit  \\\n",
      "0  gh1dj9  [Project] From books to presentations in 10s w...  MachineLearning   \n",
      "1  gh1dj9  [Project] From books to presentations in 10s w...  MachineLearning   \n",
      "2  gh1dj9  [Project] From books to presentations in 10s w...  MachineLearning   \n",
      "3  gh1dj9  [Project] From books to presentations in 10s w...  MachineLearning   \n",
      "4  gh1dj9  [Project] From books to presentations in 10s w...  MachineLearning   \n",
      "\n",
      "                          post_url flair_text  score  comments  upvote_ratio  \\\n",
      "0  https://v.redd.it/v492uoheuxx41    Project   7798       186          0.99   \n",
      "1  https://v.redd.it/v492uoheuxx41    Project   7798       186          0.99   \n",
      "2  https://v.redd.it/v492uoheuxx41    Project   7798       186          0.99   \n",
      "3  https://v.redd.it/v492uoheuxx41    Project   7798       186          0.99   \n",
      "4  https://v.redd.it/v492uoheuxx41    Project   7798       186          0.99   \n",
      "\n",
      "             date-time  year  \\\n",
      "0  2020-05-10 13:19:54  2020   \n",
      "1  2020-05-10 13:19:54  2020   \n",
      "2  2020-05-10 13:19:54  2020   \n",
      "3  2020-05-10 13:19:54  2020   \n",
      "4  2020-05-10 13:19:54  2020   \n",
      "\n",
      "                                             comment  \n",
      "0  Twitter thread: [https://twitter.com/cyrildiag...  \n",
      "1                                       The future ðŸ¤¯  \n",
      "2  Simple yet very useful. Thank you for sharing ...  \n",
      "3  Almost guaranteed, Apple will copy your idea i...  \n",
      "4  Ohh the nightmare of making this into a stable...  \n"
     ]
    }
   ],
   "source": [
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e592e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datascience        107156\n",
      "MachineLearning     95702\n",
      "artificial          20310\n",
      "Name: subreddit, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_df['subreddit'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b060ba2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_id             0\n",
      "post_title          0\n",
      "subreddit           0\n",
      "post_url            0\n",
      "flair_text      24738\n",
      "score               0\n",
      "comments            0\n",
      "upvote_ratio        0\n",
      "date-time           0\n",
      "year                0\n",
      "comment             9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d04d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop('flair_text', axis=1)\n",
    "#dropped flair because it had missing values in it and I though it would be best to drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e5ff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  post_id                                         post_title        subreddit  \\\n",
      "0  gh1dj9  [Project] From books to presentations in 10s w...  MachineLearning   \n",
      "1  gh1dj9  [Project] From books to presentations in 10s w...  MachineLearning   \n",
      "2  gh1dj9  [Project] From books to presentations in 10s w...  MachineLearning   \n",
      "3  gh1dj9  [Project] From books to presentations in 10s w...  MachineLearning   \n",
      "4  gh1dj9  [Project] From books to presentations in 10s w...  MachineLearning   \n",
      "\n",
      "                          post_url  score  comments  upvote_ratio  \\\n",
      "0  https://v.redd.it/v492uoheuxx41   7798       186          0.99   \n",
      "1  https://v.redd.it/v492uoheuxx41   7798       186          0.99   \n",
      "2  https://v.redd.it/v492uoheuxx41   7798       186          0.99   \n",
      "3  https://v.redd.it/v492uoheuxx41   7798       186          0.99   \n",
      "4  https://v.redd.it/v492uoheuxx41   7798       186          0.99   \n",
      "\n",
      "             date-time  year  \\\n",
      "0  2020-05-10 13:19:54  2020   \n",
      "1  2020-05-10 13:19:54  2020   \n",
      "2  2020-05-10 13:19:54  2020   \n",
      "3  2020-05-10 13:19:54  2020   \n",
      "4  2020-05-10 13:19:54  2020   \n",
      "\n",
      "                                             comment  \n",
      "0  Twitter thread: [https://twitter.com/cyrildiag...  \n",
      "1                                       The future ðŸ¤¯  \n",
      "2  Simple yet very useful. Thank you for sharing ...  \n",
      "3  Almost guaranteed, Apple will copy your idea i...  \n",
      "4  Ohh the nightmare of making this into a stable...  \n",
      "Index(['post_id', 'post_title', 'subreddit', 'post_url', 'score', 'comments',\n",
      "       'upvote_ratio', 'date-time', 'year', 'comment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.head())\n",
    "print(merged_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f314eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create an instance of the PorterStemmer and WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text into individual words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords from the text\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords_list]\n",
    "    \n",
    "    # Apply stemming or lemmatization to the tokens\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    # Return the preprocessed text as a string\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply the preprocess_text function to the 'comment' column of your DataFrame\n",
    "merged_df['preprocessed_comment'] = merged_df['comment'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0965a6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['comment'].isnull().sum()\n",
    "# 9 missing values in the comment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42a4cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dropna(subset=['comment'], inplace=True)\n",
    "# we drop the missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c69e644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Twitter thread: [https://twitter.com/cyrildiag...\n",
      "1                                         The future ðŸ¤¯\n",
      "2    Simple yet very useful. Thank you for sharing ...\n",
      "3    Almost guaranteed, Apple will copy your idea i...\n",
      "4    Ohh the nightmare of making this into a stable...\n",
      "Name: comment, dtype: object\n",
      "0    twitter thread : [ http : //twitter.com/cyrild...\n",
      "1                                             future ðŸ¤¯\n",
      "2             simple yet useful . thank sharing code .\n",
      "3    almost guaranteed , apple copy idea 3 , 2 , 1 ...\n",
      "4    ohh nightmare making stable product ... enough...\n",
      "Name: preprocessed_comment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print a sample of the original 'comment' column\n",
    "print(merged_df['comment'].head())\n",
    "\n",
    "# Print a sample of the 'preprocessed_comment' column after applying the 'preprocess_text' function\n",
    "print(merged_df['preprocessed_comment'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f160292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged_df['preprocessed_comment'], merged_df['subreddit'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training data further into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4989457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185231    airflow allows write data pipeline python . ma...\n",
      "146600    hi prof. hinton , 'd like thank introduction m...\n",
      "90000     > 's inappropriate try benefit also trying exe...\n",
      "25033                                  france popular app .\n",
      "152800                prefer word `` differently abled '' .\n",
      "                                ...                        \n",
      "181937    ingest : load postgresql via psql , transform ...\n",
      "156967    project work algorithm research generative art...\n",
      "62417                                                      \n",
      "12413                        one task left jira : fix human\n",
      "130565    unfortunately post feed onto current `` china ...\n",
      "Name: preprocessed_comment, Length: 142821, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "145ca8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for validation set:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "MachineLearning       0.68      0.71      0.69     15214\n",
      "     artificial       0.79      0.00      0.01      3251\n",
      "    datascience       0.73      0.84      0.78     17241\n",
      "\n",
      "       accuracy                           0.71     35706\n",
      "      macro avg       0.73      0.52      0.49     35706\n",
      "   weighted avg       0.71      0.71      0.67     35706\n",
      "\n",
      "Classification report for test set:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "MachineLearning       0.69      0.70      0.69     19193\n",
      "     artificial       0.92      0.00      0.01      4044\n",
      "    datascience       0.72      0.84      0.78     21395\n",
      "\n",
      "       accuracy                           0.71     44632\n",
      "      macro avg       0.78      0.52      0.49     44632\n",
      "   weighted avg       0.72      0.71      0.67     44632\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert the preprocessed comments into numerical feature vectors using the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_val_vectorized = vectorizer.transform(X_val)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier on the training set\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the validation set and print the classification report\n",
    "y_pred_val = mnb.predict(X_val_vectorized)\n",
    "print('Classification report for validation set:')\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "# Make predictions on the test set and print the classification report\n",
    "y_pred_test = mnb.predict(X_test_vectorized)\n",
    "print('Classification report for test set:')\n",
    "print(classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c00d045",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       original_subreddit predicted_subreddit\n",
      "202182        datascience         datascience\n",
      "162932         artificial     MachineLearning\n",
      "43317         datascience         datascience\n",
      "198271        datascience         datascience\n",
      "135856        datascience         datascience\n",
      "202604        datascience         datascience\n",
      "176299        datascience         datascience\n",
      "155230    MachineLearning         datascience\n",
      "215428         artificial         datascience\n",
      "159986    MachineLearning     MachineLearning\n",
      "183599        datascience         datascience\n",
      "8931      MachineLearning     MachineLearning\n",
      "112367        datascience     MachineLearning\n",
      "208699         artificial     MachineLearning\n",
      "174199        datascience         datascience\n",
      "186700        datascience     MachineLearning\n",
      "118183    MachineLearning     MachineLearning\n",
      "40354     MachineLearning     MachineLearning\n",
      "47130     MachineLearning     MachineLearning\n",
      "59464         datascience         datascience\n",
      "59917     MachineLearning     MachineLearning\n",
      "53782     MachineLearning         datascience\n",
      "217782         artificial         datascience\n",
      "40303          artificial     MachineLearning\n",
      "190438        datascience         datascience\n",
      "54030     MachineLearning     MachineLearning\n",
      "180254        datascience         datascience\n",
      "86294     MachineLearning     MachineLearning\n",
      "23888     MachineLearning     MachineLearning\n",
      "180447        datascience     MachineLearning\n",
      "169230    MachineLearning         datascience\n",
      "149771        datascience         datascience\n",
      "53681         datascience         datascience\n",
      "56412         datascience         datascience\n",
      "64812         datascience         datascience\n",
      "151975    MachineLearning     MachineLearning\n",
      "142397        datascience         datascience\n",
      "87012         datascience     MachineLearning\n",
      "108308        datascience         datascience\n",
      "159412        datascience     MachineLearning\n",
      "12765         datascience     MachineLearning\n",
      "60373     MachineLearning     MachineLearning\n",
      "145525        datascience         datascience\n",
      "209246         artificial     MachineLearning\n",
      "118239    MachineLearning     MachineLearning\n",
      "126257    MachineLearning     MachineLearning\n",
      "41224         datascience         datascience\n",
      "20928     MachineLearning         datascience\n",
      "184228        datascience         datascience\n",
      "8082          datascience     MachineLearning\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = mnb.predict(X_test_vectorized)\n",
    "\n",
    "# Create a new DataFrame with the original subreddit and predicted subreddit columns\n",
    "results_df = pd.DataFrame({'original_subreddit': y_test, 'predicted_subreddit': y_pred})\n",
    "\n",
    "# Print the first 10 rows of the results DataFrame\n",
    "print(results_df.head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09393218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7058388600107546\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = mnb.predict(X_test_vectorized)\n",
    "\n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
