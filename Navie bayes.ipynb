{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f20d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# Load the two CSV files into separate DataFrames\n",
    "posts_df = pd.read_csv('Top_posts.csv')\n",
    "comments_df = pd.read_csv('Top_posts_comments.csv')\n",
    "\n",
    "# Drop the 'flair_text' column\n",
    "posts_df = posts_df.drop('flair_text', axis=1)\n",
    "\n",
    "# Merge the two DataFrames on the post ID column\n",
    "merged_df = pd.merge(posts_df, comments_df, on='post_id')\n",
    "\n",
    "merged_df.dropna(subset=['comment'], inplace=True)\n",
    "# we drop the missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4724da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply the preprocessing function to the 'comment' column of your DataFrame\n",
    "merged_df['preprocessed_comment'] = merged_df['comment'].apply(preprocess_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd37f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['preprocessed_comment'] = merged_df['comment'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cdec165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(texts, min_count=1):\n",
    "    # Create an empty Counter object\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    # Loop over each text and count the frequency of each word\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            vocabulary[word] += 1\n",
    "\n",
    "    # Return the vocabulary as a list of words, sorted by frequency\n",
    "    return [word for word, freq in vocabulary.most_common() if freq >= min_count]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3702cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary from the preprocessed comments\n",
    "vocabulary = create_vocabulary(merged_df['preprocessed_comment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b1d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', ',', ')', '(', '?', \"'s\", \"n't\", ':', '*', 'data']\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary[:10])\n",
    "#first 10 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e981661",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m         vocabulary\u001b[38;5;241m.\u001b[39madd(word)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Create a bag of words for each preprocessed comment\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbag_of_words\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpreprocessed_comment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_bag_of_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     15\u001b[0m         vocabulary\u001b[38;5;241m.\u001b[39madd(word)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Create a bag of words for each preprocessed comment\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbag_of_words\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_comment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mcreate_bag_of_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mcreate_bag_of_words\u001b[1;34m(text, vocabulary)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m vocabulary:\n\u001b[0;32m      8\u001b[0m         bag_of_words\u001b[38;5;241m.\u001b[39mpop(word)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[43m[\u001b[49m\u001b[43mbag_of_words\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m vocabulary:\n\u001b[0;32m      8\u001b[0m         bag_of_words\u001b[38;5;241m.\u001b[39mpop(word)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([bag_of_words\u001b[38;5;241m.\u001b[39mget(word, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m vocabulary])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def create_bag_of_words(text, vocabulary):\n",
    "    bag_of_words = Counter(text.split())\n",
    "    for word in set(text.split()):\n",
    "        if word not in vocabulary:\n",
    "            bag_of_words.pop(word)\n",
    "    return np.array([bag_of_words.get(word, 0) for word in vocabulary])\n",
    "\n",
    "# Create a list of all unique words in your dataset\n",
    "vocabulary = set()\n",
    "for comment in merged_df['preprocessed_comment']:\n",
    "    for word in comment.split():\n",
    "        vocabulary.add(word)\n",
    "\n",
    "# Create a bag of words for each preprocessed comment\n",
    "merged_df['bag_of_words'] = merged_df['preprocessed_comment'].apply(lambda x: create_bag_of_words(x, vocabulary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d87947",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mdata\u001b[49m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_title\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubreddit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_url\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflair_text\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomments\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupvote_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate-time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_comment\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbag_of_words\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_comment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: create_bag_of_words(x, vocabulary))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(data, columns=['post_id', 'post_title', 'subreddit', 'post_url', 'flair_text', 'score',\n",
    "       'comments', 'upvote_ratio', 'date-time', 'year', 'comment',\n",
    "       'preprocessed_comment'])\n",
    "\n",
    "data['bag_of_words'] = data['preprocessed_comment'].apply(lambda x: create_bag_of_words(x, vocabulary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755b58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# Convert the list of bag of words to a sparse matrix\n",
    "rows = []\n",
    "cols = []\n",
    "data = []\n",
    "for i, row in enumerate(bag_of_words_list):\n",
    "    for j, value in enumerate(row):\n",
    "        if value != 0:\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(value)\n",
    "bag_of_words_matrix = coo_matrix((data, (rows, cols)), shape=(len(bag_of_words_list), len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_size = int(len(merged_df) * 0.8)\n",
    "train_data = merged_df[:train_size]\n",
    "test_data = merged_df[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee2ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train the Naive Bayes classifier\n",
    "def train_naive_bayes(train_data):\n",
    "    # Count the number of comments in each class\n",
    "    class_counts = Counter(train_data['score'])\n",
    "    \n",
    "    # Calculate the total number of comments\n",
    "    total_comments = len(train_data)\n",
    "    \n",
    "    # Create an empty dictionary to store the conditional probabilities\n",
    "    cond_probs = {}\n",
    "    \n",
    "    # Loop over each class\n",
    "    for class_label in class_counts.keys():\n",
    "        # Get the subset of comments with the current class label\n",
    "        class_data = train_data[train_data['score'] == class_label]\n",
    "        \n",
    "        # Count the number of comments in the current class\n",
    "        class_comment_count = len(class_data)\n",
    "        \n",
    "        # Calculate the prior probability of the current class\n",
    "        prior_prob = class_counts[class_label] / total_comments\n",
    "        \n",
    "        # Create an empty dictionary to store the conditional probabilities for the current class\n",
    "        cond_probs_for_class = {}\n",
    "        \n",
    "        # Loop over each word in the vocabulary\n",
    "        for word in vocabulary:\n",
    "            # Count the number of comments in the current class that contain the current word\n",
    "            word_count = sum(class_data['bag_of_words'].apply(lambda x: 1 if x[vocabulary.index(word)] > 0 else 0))\n",
    "            \n",
    "            # Calculate the conditional probability of the current word given the current class\n",
    "            cond_prob = (word_count + 1) / (class_comment_count + len(vocabulary))\n",
    "            \n",
    "            # Add the conditional probability to the dictionary\n",
    "            cond_probs_for_class[word] = cond_prob\n",
    "        \n",
    "        # Add the dictionary of conditional probabilities for the current class to the overall dictionary\n",
    "        cond_probs[class_label] = cond_probs_for_class\n",
    "    \n",
    "    # Return the prior probabilities and conditional probabilities\n",
    "    return class_counts, prior_prob, cond_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify a comment using the Naive Bayes classifier\n",
    "def classify_comment(comment, class_counts, prior_probs, cond_probs):\n",
    "    # Create an empty dictionary to store the log probabilities for each class\n",
    "    log_probs = {}\n",
    "    \n",
    "    # Loop over each class\n",
    "    for class_label in class_counts.keys():\n",
    "        # Initialize the log probability with the logarithm of the prior probability\n",
    "        log_prob = np.log(prior_probs[class_label])\n",
    "        \n",
    "        # Loop over each word in the comment\n",
    "        for word in comment:\n",
    "            # Check if the word is in the vocabulary\n",
    "            if word in vocabulary:\n",
    "                # Add the logarithm of the conditional probability to the log probability\n",
    "                log_prob += np.log(cond_probs[class_label][word])\n",
    "        \n",
    "        # Add the log probability to the dictionary\n",
    "        log_probs[class_label] = log_prob\n",
    "    \n",
    "    # Return the class label with the highest log probability\n",
    "    return max(log_probs, key=log_probs.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840edc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f500a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Naive Bayes classifier\n",
    "class_counts, prior_probs, cond_probs = train_naive_bayes(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the test comments\n",
    "test_data['predicted_score'] = test_data['preprocessed_comment'].apply(lambda x: classify_comment(x, class_counts, prior_probs, cond_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59818de2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Print the classification report\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(\u001b[43mtest_data\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m], test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_score\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the classification report\n",
    "print(classification_report(test_data['score'], test_data['predicted_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c162998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_comments(test_data, class_counts, prior_prob, cond_probs):\n",
    "    # Create an empty list to store the predicted class labels\n",
    "    predicted_labels = []\n",
    "    \n",
    "    # Loop over each comment in the test data\n",
    "    for comment in test_data['preprocessed_comment']:\n",
    "        # Create an empty dictionary to store the posterior probabilities for each class\n",
    "        posteriors = {}\n",
    "        \n",
    "        # Loop over each class\n",
    "        for class_label in class_counts.keys():\n",
    "            # Get the prior probability for the current class\n",
    "            prior = prior_prob[class_label]\n",
    "            \n",
    "            # Get the dictionary of conditional probabilities for the current class\n",
    "            cond_probs_for_class = cond_probs[class_label]\n",
    "            \n",
    "            # Calculate the likelihood of the comment given the current class\n",
    "            likelihood = 1\n",
    "            for word in comment.split():\n",
    "                likelihood *= cond_probs_for_class.get(word, 1)\n",
    "            \n",
    "            # Calculate the posterior probability of the current class\n",
    "            posterior = prior * likelihood\n",
    "            \n",
    "            # Add the posterior probability to the dictionary\n",
    "            posteriors[class_label] = posterior\n",
    "        \n",
    "        # Determine the predicted class label for the current comment\n",
    "        predicted_label = max(posteriors, key=posteriors.get)\n",
    "        \n",
    "        # Add the predicted class label to the list\n",
    "        predicted_labels.append(predicted_label)\n",
    "    \n",
    "    # Return the list of predicted class labels\n",
    "    return predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac2f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
